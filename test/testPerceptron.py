#!/usr/bin/env python3
# coding=utf-8
# import
from unittest import TestCase
from random import randint, choice
from string import ascii_letters
from neuralnetworkcommon.perceptron import Perceptron, Layer, Sigmoid
# test perceptron
class testPerceptron(TestCase):
    # test sigmoid computing
    def testSigmoidValue(self):
        variables = [-4.759797336992447, 5.224268412298162, -0.8238109165048839, -4.335650860536461, -12.139774266734126, -3.9137186632355467, 2.3090778436678105, -2.6633803360485158, 4.912045083504539, -10.442216563757164]
        expectedValue = [0.008494569592952588, 0.9946445377970348, 0.30495530742883076, 0.012924129215778584, 5.342698960038116e-06, 0.019575273408768638, 0.9096260767034267, 0.0651690933628563, 0.9926963099778543, 2.9173618362790917e-05]
        actualValue = Sigmoid.value(variables)
        actualValue = [float(_) for _ in actualValue]
        self.assertListEqual(expectedValue, actualValue, "ERROR : sigmoïd value does not match")
        pass
    def testSigmoidDerivative(self):
        variables = [0.024861673929273992, 0.28109464441649784, 4.844212871290259e-07, 0.8458432780821669, 0.9974294986897912, 0.5396884426206188, 0.11165856728006232, 0.9982761962289739, 0.9706007225176077, 0.003034795109527507]
        expectedValue = [0.02424357109870845, 0.20208044529686048, 4.844210524650425e-07, 0.130392427005381, 0.0025638938332229657, 0.24842482752234984, 0.09919093163302611, 0.0017208322715850963, 0.02853495996590565, 0.003025585128170695]
        actualValue = Sigmoid.derivative(variables)
        actualValue = [float(_) for _ in actualValue]
        self.assertListEqual(expectedValue, actualValue, "ERROR : sigmoïd value does not match")
        pass
    # test layer computing
    def testDifferentialErrorOutput(self):
        expectedOutput = [0,0,1,0,0,0,0,0,0,0]
        actualOutput = [0.9995057415494378, 0.0005696693299897813, 6.773856462448391e-07, 0.7129941025529047, 0.008378539587771152, 0.0006705591635133233, 0.9652315201365399, 2.07564711785314e-05, 0.984934124954549, 0.0001956411273144735]
        expectedDifferentialError = [0.9995057415494378, 0.0005696693299897813, -0.9999993226143538, 0.7129941025529047, 0.008378539587771152, 0.0006705591635133233, 0.9652315201365399, 2.07564711785314e-05, 0.984934124954549, 0.0001956411273144735]
        actualDifferentialError = Layer.differentialErrorOutput(actualOutput,expectedOutput)
        actualDifferentialError = [float(_) for _ in actualDifferentialError]
        self.assertListEqual(expectedDifferentialError, actualDifferentialError, "ERROR : differential error output does not match")
        pass
    def testDifferentialErrorHidden(self):
        previousDifferentielError = [[0.14183407658948294],[0.14464286433485501],[0.12703726353950687],[0.08500328181266716],[0.09355259185226361],[0.10454788124775924],[0.07756524761984694],[0.033814213071683974],[-0.08176974843303718],[0.0825255766755945]]
        previousLayerWeights = [[-0.7339338396373078,-0.09171190968689302,0.9122745639261545,-0.8513173916984993,-0.8045567236137197,0.3760256831987825,-0.4882869188388077,-0.16430627283202415,-0.1477107907164379,0.6159008535197281,-0.7252812201318208,0.8132571694018083,0.12669455874542357,0.6472288737673857,0.26026542509041617],
            [-0.055150887866475706,-0.8670206367372424,0.0432210688836796,-0.7654661904221913,-0.9519694741261651,0.7522244736756616,0.6025507123354281,0.1853438832690888,-0.0461187754732737,-0.8627466418940204,-0.22009605272489363,0.513250107777429,-0.10882399748643556,0.8558924750865808,0.1640716588563096],
            [0.08807173146658442,0.7782244713793252,-0.9049346091312462,-0.23149227613708634,0.7149975656557102,-0.4999185332254117,0.21375858297494577,0.5866733028777451,0.8761531173193273,0.15397435305961582,-0.9457933332575676,0.39740420778464913,-0.9519379825765892,0.974233426263658,0.6275922245268184],
            [-0.35229939394358234,0.04799057478881452,0.5065128796329623,-0.60469030881464,0.3850504832667181,-0.06417838203463022,-0.8380561619707931,-0.2544993248214522,0.9477148323884432,-0.14902540028474576,0.8919620606403573,0.9223810830236232,-0.2449829632304652,0.9844058272694305,0.908219823522453],
            [-0.05893307437054185,0.5653770226752051,0.3499375315776432,-0.05067925307220578,0.4109552262513123,-0.5700091948879229,0.009649303756931182,-0.6143638270547374,-0.12709619535169003,0.5389043627792118,0.8636496840894341,-0.14288134339112202,0.2812826035570566,-0.5120400688698492,0.11462549144076317],
            [0.1768992855106295,-0.19185808872390253,-0.9236792411331485,-0.5704488327714192,0.49854122580540716,0.5788998407974435,0.9616383384984102,0.6033732403517966,0.6368617502438978,-0.28147250927263334,-0.5262111582015967,0.5360724428034356,-0.5506641586920458,0.591432065606339,0.6448246367795463],
            [-0.7109557836492975,0.5584818622758692,-0.6050828876131416,-0.03178242450170976,-0.383216859999747,-0.24615279149327574,0.5700969741818207,-0.1816139598841513,-0.47423686753531324,0.46998566986474066,-0.7952330413917452,0.5568353416661986,-0.5953101741319509,0.10253030827262233,0.8585439385437519],
            [0.9114487425713862,0.10385893412198066,-0.1538672765111444,0.7458496238727699,0.617931339253667,0.8020821338080604,0.38310696347715334,0.8337150723636286,0.4886782877599145,0.2414930614909696,0.4977770400313333,-0.7897929309964329,-0.5100598380863965,-0.05727040343051226,0.8080436203800161],
            [-0.020349060641251526,0.37483041922364313,-0.5086020357853125,-0.7469501680019115,0.16043699491118746,-0.4729718484291232,0.7074495870118833,0.1848059800017341,-0.8991298935502758,-0.9898446405799668,-0.4852331868857338,-0.3554407266661139,0.20005740942522876,0.027798910604015514,-0.5129507030526008],
            [0.07380317736892184,-0.7727682521787156,0.06536782070630753,0.9021452176115055,0.4744062246665979,-0.6868301984179919,-0.47873278251061335,-0.6365980946218983,0.9018052040566951,0.001548827777451356,0.11302299591561349,-0.11854790648478941,-0.3615901923193121,-0.9353053023398443,0.6587683945732208]]
        expectedDifferentialError = [-0.1344220721562209,-0.05023147223870325,-0.00524659161353101,-0.21837114866269153,-0.020477601224295115,0.09039302155467922,0.03507518596878928,0.008463924322890157,0.3466177496028463,0.11613457899661646,-0.14909963076277233,0.3969203348293972,-0.2804047519432634,0.3635263328280953,0.4859397506679662]
        actualDifferentialError = Layer.differentialErrorHidden(previousDifferentielError,previousLayerWeights)
        actualDifferentialError = [float(_) for _ in actualDifferentialError]
        self.assertListEqual(expectedDifferentialError, actualDifferentialError, "ERROR : differential error hidden does not match")
        pass
    def testcomputeNewWeights(self):
        layer = Layer()
        layer.weights = [[-0.18142498326592915, 0.9127768930680651, 0.5851490152637835, 0.39951750591964763, -0.8716538097989281, 0.4555400718117186, -0.610623752160611, 0.768491278859532, 0.06183775388531654, -0.2864465501931208, -0.6391872952529665, -0.6779920943278348, 0.4891262144899948, -0.37370931627975423, -0.7773139411576371], [-0.11680081193569447, -0.3160450959531458, -0.9223320879555124, -0.16775837158120255, -0.6768736571610854, -0.1445971140289921, 0.01167888651935689, -0.7781563970805092, -0.22517500311351224, -0.6089663022517275, 0.376438052237841, -0.5435477877050503, 0.6720276277108008, -0.04301812061606625, 0.94173884592829], [0.1942698798535556, 0.6376162039655764, 0.71053292009958, 0.6400281855776488, 0.6469370900562907, -0.2363703394533443, 0.6757435436655064, 0.5933365607124284, 0.23095506337174987, 0.5430780824994483, 0.14453597705026744, -0.5781102335492665, -0.982664193083902, 0.3520637846792978, -0.7370802005471044], [-0.5775300538084585, -0.06310854848894332, 0.9678974937470828, 0.5487276350974375, -0.5780868491199977, -0.11634409521547551, -0.18429134387864443, -0.40170215693685685, 0.6408946728410845, -0.24896027654774144, -0.16132650149051053, -0.7931090154666871, 0.16630856749303424, -0.18441515961680666, 0.4343220720147791], [0.8312197934211241, -0.821437854356905, -0.355038182828183, 0.09381347813280771, -0.6043973699872462, 0.22421016163519836, -0.05482488615390335, 0.562569084963275, -0.6556513216666049, 0.5280116061098659, 0.8747902174640139, 0.9690163202397224, 0.7294842133764052, 0.5180547297923381, -0.1638302703203709], [-0.8608210052420062, 0.4919018940803861, -0.2652351554162353, 0.5384945740505664, -0.8579008569043578, 0.13593115872353634, 0.07749265390707838, -0.43079133053318186, 0.8229745851497567, 0.7560906371222582, -0.9029059192789832, 0.48519472007656805, 0.9766202363408276, 0.4752497895536021, 0.31945051160861904], [0.44128314318366657, 0.8346610341396541, 0.0597449852492713, 0.008960768114841144, 0.4820615242024071, -0.3847997925377238, -0.7881980710174932, -0.7284227575586024, 0.5211234177601454, -0.04037602778648708, 0.0744443725898003, 0.9415438246100247, 0.7259741986212171, -0.5717409162684384, -0.12159581440556533], [0.440349691898426, 0.7336986447526184, -0.041863763504363494, -0.4817000007754415, -0.26403905559892826, 0.8835060503992918, -0.441973274621158, 0.051184066574523124, 0.6241419194419939, -0.5082698430597317, -0.014751661331267929, -0.19840397149590827, 0.31484439375058937, -0.29821301267419154, -0.7845823717258789], [-0.8152060425706078, 0.5738854756129557, 0.7074371380518785, -0.8398184611819199, -0.7886414126013837, -0.846935562796663, -0.8135812806731575, -0.7255426127847422, -0.8241927366999717, -0.0002311314202241288, 0.38065791411421457, 0.9352890568542571, -0.147753202738222, -0.40734259915346205, -0.8098591352504529], [-0.24088715377343028, -0.5606111777606673, 0.5967429228847703, 0.8563145762329876, 0.21846139360838612, 0.9972664882533735, 0.4993637497231742, 0.35807274267266465, -0.8850009611116814, 0.7598389246618025, -0.05366822253515613, 0.5037304193545071, -0.5538095239081038, 0.01336084980267871, 0.39131839946764524]]
        input = (0.9462496492963102, 0.050016409581345726, 0.16211520684811256, 0.9879479779545183, 0.7582877511461974, 0.920878836285209, 0.2512672673372151, 0.09371770756475498, 0.6010634260699207, 0.1216037807230065, 0.9271263346043986, 0.1394034503218345, 0.08927296349194723, 0.367548752957228, 0.24046569912831703)
        output = [0.28399601541553166, 0.30501484366651005, 0.8298014162600169, 0.41732840297544566, 0.8021958754167957, 0.33372395002539135, 0.6356983021908221, 0.6176687459559471, 0.029949608721962142, 0.8263960019779597]
        # INFO : differentialErrorLayer!=output (index 7 : 0.6176687459559471!=-0.3823312540440529)
        differentialErrorLayer = [0.28399601541553166, 0.30501484366651005, 0.8298014162600169, 0.41732840297544566, 0.8021958754167957, 0.33372395002539135, 0.6356983021908221, -0.3823312540440529, 0.029949608721962142, 0.8263960019779597]
        expectedNewDifferentialErrorWeightsBiases = [[0.05774839690030647], [0.06465728715906499], [0.1171937052559847], [0.10147983098113503], [0.1272905586620176], [0.07420427957849257], [0.1472188084376666], [-0.09028908028746574], [0.0008701148906420094], [0.1185594394925827]]
        expectedOldWeights = [[-0.18142498326592915, 0.9127768930680651, 0.5851490152637835, 0.39951750591964763, -0.8716538097989281, 0.4555400718117186, -0.610623752160611, 0.768491278859532, 0.06183775388531654, -0.2864465501931208, -0.6391872952529665, -0.6779920943278348, 0.4891262144899948, -0.37370931627975423, -0.7773139411576371], [-0.11680081193569447, -0.3160450959531458, -0.9223320879555124, -0.16775837158120255, -0.6768736571610854, -0.1445971140289921, 0.01167888651935689, -0.7781563970805092, -0.22517500311351224, -0.6089663022517275, 0.376438052237841, -0.5435477877050503, 0.6720276277108008, -0.04301812061606625, 0.94173884592829], [0.1942698798535556, 0.6376162039655764, 0.71053292009958, 0.6400281855776488, 0.6469370900562907, -0.2363703394533443, 0.6757435436655064, 0.5933365607124284, 0.23095506337174987, 0.5430780824994483, 0.14453597705026744, -0.5781102335492665, -0.982664193083902, 0.3520637846792978, -0.7370802005471044], [-0.5775300538084585, -0.06310854848894332, 0.9678974937470828, 0.5487276350974375, -0.5780868491199977, -0.11634409521547551, -0.18429134387864443, -0.40170215693685685, 0.6408946728410845, -0.24896027654774144, -0.16132650149051053, -0.7931090154666871, 0.16630856749303424, -0.18441515961680666, 0.4343220720147791], [0.8312197934211241, -0.821437854356905, -0.355038182828183, 0.09381347813280771, -0.6043973699872462, 0.22421016163519836, -0.05482488615390335, 0.562569084963275, -0.6556513216666049, 0.5280116061098659, 0.8747902174640139, 0.9690163202397224, 0.7294842133764052, 0.5180547297923381, -0.1638302703203709], [-0.8608210052420062, 0.4919018940803861, -0.2652351554162353, 0.5384945740505664, -0.8579008569043578, 0.13593115872353634, 0.07749265390707838, -0.43079133053318186, 0.8229745851497567, 0.7560906371222582, -0.9029059192789832, 0.48519472007656805, 0.9766202363408276, 0.4752497895536021, 0.31945051160861904], [0.44128314318366657, 0.8346610341396541, 0.0597449852492713, 0.008960768114841144, 0.4820615242024071, -0.3847997925377238, -0.7881980710174932, -0.7284227575586024, 0.5211234177601454, -0.04037602778648708, 0.0744443725898003, 0.9415438246100247, 0.7259741986212171, -0.5717409162684384, -0.12159581440556533], [0.440349691898426, 0.7336986447526184, -0.041863763504363494, -0.4817000007754415, -0.26403905559892826, 0.8835060503992918, -0.441973274621158, 0.051184066574523124, 0.6241419194419939, -0.5082698430597317, -0.014751661331267929, -0.19840397149590827, 0.31484439375058937, -0.29821301267419154, -0.7845823717258789], [-0.8152060425706078, 0.5738854756129557, 0.7074371380518785, -0.8398184611819199, -0.7886414126013837, -0.846935562796663, -0.8135812806731575, -0.7255426127847422, -0.8241927366999717, -0.0002311314202241288, 0.38065791411421457, 0.9352890568542571, -0.147753202738222, -0.40734259915346205, -0.8098591352504529], [-0.24088715377343028, -0.5606111777606673, 0.5967429228847703, 0.8563145762329876, 0.21846139360838612, 0.9972664882533735, 0.4993637497231742, 0.35807274267266465, -0.8850009611116814, 0.7598389246618025, -0.05366822253515613, 0.5037304193545071, -0.5538095239081038, 0.01336084980267871, 0.39131839946764524]]
        expectedNewWeights = [[-0.20874718342309873, 0.9113327093320491, 0.5804680686094634, 0.37099129994576124, -0.8935487608078438, 0.4289504335442733, -0.6178788931017335, 0.7657852551730138, 0.04448252923984464, -0.2899577618900058, -0.665957325026697, -0.6820172572170635, 0.48654852922589503, -0.3843219919127476, -0.784257195474723], [-0.1473917795850525, -0.317662058631629, -0.9275730526965271, -0.199697389635614, -0.701388071598616, -0.17436787770719128, 0.0035557565904089563, -0.781186163445461, -0.24460656838361938, -0.6128975875366452, 0.346465315413217, -0.5480545121642619, 0.6691415538927811, -0.05490047324852211, 0.9339649160480676], [0.13882262860445027, 0.634685399784357, 0.7010334792151447, 0.5821375435093249, 0.6025038144527652, -0.2903309409113858, 0.6610200726311092, 0.5878449980136231, 0.19573463837425456, 0.5359524836814156, 0.09020929185392276, -0.5862788369836087, -0.9878953077693038, 0.33052658456866074, -0.7511707336810136], [-0.6255426810467324, -0.06564637688424241, 0.9596717818518736, 0.4985992381869482, -0.6165623055306882, -0.16306940954563917, -0.19704062378888057, -0.4064573854986622, 0.6103967653978257, -0.2551304421049603, -0.20836881335741733, -0.8001823347551105, 0.16177886486986032, -0.2030645522805199, 0.42212086276262806], [0.7709954701747913, -0.8246211627158438, -0.3653560504518354, 0.030935253111387076, -0.6526588057222285, 0.16560057086981192, -0.07081686157031968, 0.5566043952870543, -0.6939061713144782, 0.5202720995180433, 0.8157830029229826, 0.9601439487042829, 0.7238024106782533, 0.4946619867426114, -0.17913477691091895], [-0.8959288920057232, 0.4900461782603428, -0.27124997648267657, 0.5018395900679947, -0.8860349550478572, 0.10176458341072407, 0.06817010062987111, -0.4342684580199771, 0.8006738458935072, 0.7515788766509723, -0.9373042901477672, 0.4800225537756241, 0.9733080183699491, 0.4616129443420199, 0.31052871962504136], [0.3716302702566852, 0.8309793560292061, 0.047811781458368806, -0.063761493941592, 0.42624441461409657, -0.45258513503441056, -0.8066937048658799, -0.7353212621771988, 0.47687949706940774, -0.04932720963626523, 0.006199155463979825, 0.9312824196857847, 0.7194028689657253, -0.5987959609949951, -0.13929635125346596], [0.4830676971770764, 0.7359566125628089, -0.03454514704089932, -0.43709954362475406, -0.22980650377680784, 0.9250787019914833, -0.43062992938404704, 0.05541490938585882, 0.651276651409137, -0.5027800962992522, 0.02710303069959226, -0.19211066683667935, 0.3188745806347016, -0.281620193241535, -0.7737266583183897], [-0.8156177155256665, 0.5738637156015791, 0.7073666086241395, -0.8402482753053189, -0.7889713113332155, -0.8473361979906274, -0.813690596368578, -0.7255833853711766, -0.8244542338185936, -0.00028403605040685565, 0.38025456089959175, 0.9352284083452912, -0.14779204160565507, -0.4075025039749545, -0.809963751643203], [-0.29698056779374205, -0.5635761365033651, 0.5871327788562021, 0.7977492969759286, 0.17351030823339403, 0.9426770489180953, 0.48446869653400765, 0.3525171832329611, -0.920631832558852, 0.7526302866204532, -0.10862801181991122, 0.49546662188776275, -0.5591016001648274, -0.00842733726572464, 0.3770636602147226]]
        actualNewDifferentialErrorWeightsBiases, actualOldWeights = layer.computeNewWeights(input,output,differentialErrorLayer)
        actualNewDifferentialErrorWeightsBiases = [[float(__) for __ in _] for _ in actualNewDifferentialErrorWeightsBiases]
        actualOldWeights = [[float(__) for __ in _] for _ in actualOldWeights]
        layer.weights = [[float(__) for __ in _] for _ in layer.weights]
        self.assertListEqual(expectedNewDifferentialErrorWeightsBiases, actualNewDifferentialErrorWeightsBiases, "ERROR : NewDifferentialErrorWeightsBiases does not match")
        self.assertListEqual(expectedOldWeights, actualOldWeights, "ERROR : OldWeights does not match")
        self.assertListEqual(expectedNewWeights, layer.weights, "ERROR : NewWeights does not match")
        pass
    def testcomputeNewWeightsHidden(self):
        input = [0.9850495139393386,0.8990937319023146,0.4894580292431677,0.04424514894552958,0.0833369232101113,0.01309118385415899,0.04260004058804179,0.734036287580626,0.0918593281080508,0.44201051512685424,0.8750156839103664,0.7623921853990762,0.8916009110719337,0.9715872191936142,0.8821322604458753]
        output = []
        pass
    # test constructor
    @staticmethod
    def getRandomPerceptron():
        # randomize layers numbers, dimensions & comments
        layersNumber = randint(2,12)
        dimensions = [randint(2,100) for _ in range(layersNumber)]
        comments = "".join([choice(ascii_letters) for _ in range(15)])
        # construct perceptron
        perceptron = Perceptron.constructRandomFromDimensions(dimensions,comments)
        # return
        return perceptron, layersNumber, dimensions, comments
    def testDefaultConstructor(self):
        # random perceptron
        perceptron, layersNumber, dimensions, comments = testPerceptron.getRandomPerceptron()
        # check layers dimensions
        differentWeights = False
        perceptronLayersDimension = layersNumber-1
        self.assertEqual(perceptronLayersDimension, len(perceptron.layers), "ERROR : perceptron layers number")
        self.assertEqual(dimensions[0], len(perceptron.layers[0].weights[0]), "ERROR : first layer column dimension")
        self.assertEqual(dimensions[-1], len(perceptron.layers[-1].weights), "ERROR : last layer row dimension")
        for layerIndex in range(perceptronLayersDimension):
            firstWeight = perceptron.layers[layerIndex].weights[0][0]
            if layerIndex < perceptronLayersDimension-1 :
                nextLayerIndex = layerIndex+1
                self.assertEqual(dimensions[nextLayerIndex], len(perceptron.layers[nextLayerIndex].weights[0]), "ERROR : next layer column dimensions")
                self.assertEqual(len(perceptron.layers[layerIndex].weights), len(perceptron.layers[nextLayerIndex].weights[0]), "ERROR : layers row/column dimensions")
            rowsNumber = len(perceptron.layers[layerIndex].weights)
            self.assertEqual(perceptron.layers[layerIndex].biases, [0]*rowsNumber, "ERROR : bias != 0")
            for row in range(rowsNumber):
                for column in range(len(perceptron.layers[layerIndex].weights[0])):
                    currentWeight = perceptron.layers[layerIndex].weights[row][column]
                    self.assertGreaterEqual(currentWeight, -1, "ERROR : weight < -1")
                    self.assertLessEqual(currentWeight, 1, "ERROR : weight > 1")
                    if not differentWeights : differentWeights = firstWeight != currentWeight
                    pass
                pass
            self.assertTrue(differentWeights, "ERROR : all weights are the same")
            pass
        # check comments
        self.assertEqual(comments, perceptron.comments, "ERROR : perceptron comment")
    # test constructor
    def testPerceptronConstructorFromAttributes(self):
        # random perceptron
        initialPerceptron, _, _, _ = testPerceptron.getRandomPerceptron()
        initialPerceptron.id = randint(0, 1000)
        # construct from attributes
        constructedPerceptron = Perceptron.constructFromAttributes(initialPerceptron.id,initialPerceptron.layers,initialPerceptron.comments)
        # check construction
        self.assertTrue(initialPerceptron==constructedPerceptron, "ERROR : perceptron not consistent with attributs")
        pass
    def testLayerConstructorFromAttributes(self):
        # random layer
        previousDimension = randint(2,12)
        currentDimension = randint(2,12)
        initialLayer=Layer.constructRandomFromDimensions(previousDimension, currentDimension)
        # construct from attributes
        constructedLayer = Layer.constructFromAttributes(initialLayer.weights,initialLayer.biases)
        # check construction
        self.assertTrue(initialLayer==constructedLayer, "ERROR : layer not consistent with attributs")
        pass
    pass
pass
